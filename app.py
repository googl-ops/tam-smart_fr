#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Ù…Ù†ØµØ© ØªØ§Ù… Ø§Ù„Ø«Ù‚Ø§ÙÙŠØ© Ø§Ù„Ø°ÙƒÙŠØ© - Ø§Ù„Ø¥ØµØ¯Ø§Ø± Ø§Ù„ÙƒØ§Ù…Ù„ Ù…Ø¹ Streamlit
TAM Smart Cultural Platform - Full Version with Streamlit

Ø§Ù„Ù…Ù…ÙŠØ²Ø§Øª:
- Ø¯Ø¹Ù… Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¨Ø­ÙˆØ± (16 Ø¨Ø­Ø±)
- Ø¯Ø¹Ù… Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø£Ù†ÙˆØ§Ø¹ (ØªØ§Ù…ØŒ Ù…Ø¬Ø²ÙˆØ¡ØŒ Ù…Ø´Ø·ÙˆØ±ØŒ Ù…Ù†Ù‡ÙˆÙƒØŒ Ù…Ø±Ø¨Ø¹ØŒ Ù…Ø¶Ø§Ø±Ø¹...)
- Ø¯Ø¹Ù… Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø²Ø­Ø§ÙØ§Øª (Ø®Ø¨Ù†ØŒ Ø·ÙŠØŒ Ø¥Ù‚Ø§Ù…Ø©ØŒ ØªØ³Ù‡ÙŠÙ„ØŒ ÙƒØ³Ø±ØŒ Ø¥Ø¹Ù„Ø§Ù„ØŒ Ø¥Ø¨Ø¯Ø§Ù„)
- ØªÙ‚Ø·ÙŠØ¹ ØµÙˆØªÙŠ Ø¯Ù‚ÙŠÙ‚
- ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù‚ØµÙŠØ¯Ø© ÙƒØ§Ù…Ù„Ø©
- ØªØµØ­ÙŠØ­ ØªÙ„Ù‚Ø§Ø¦ÙŠ
- ÙˆØ§Ø¬Ù‡Ø© ØªÙØ§Ø¹Ù„ÙŠØ© Ø¬Ù…ÙŠÙ„Ø© Ù…Ø¹ Streamlit
"""

# â•â•â• ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª â•â•â•
import subprocess
import sys

def install_packages():
    packages = ['streamlit', 'pandas', 'numpy', 'plotly']
    for package in packages:
        try:
            __import__(package)
        except ImportError:
            print(f"Ø¬Ø§Ø±ÙŠ ØªØ«Ø¨ÙŠØª {package}...")
            subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", package])

install_packages()

# â•â•â• Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª â•â•â•
import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from dataclasses import dataclass, field, asdict
from typing import Dict, List, Optional, Tuple
import re
import json
from collections import defaultdict

# â•â•â• Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØµÙØ­Ø© â•â•â•
st.set_page_config(
    page_title="Ù…Ù†ØµØ© ØªØ§Ù… Ø§Ù„Ø«Ù‚Ø§ÙÙŠØ© Ø§Ù„Ø°ÙƒÙŠØ©",
    page_icon="ğŸ“œ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# â•â•â• CSS Ù…Ø®ØµØµ Ù„Ù„ØªØµÙ…ÙŠÙ… Ø§Ù„Ø¹ØµØ±ÙŠ â•â•â•
st.markdown("""
<style>
    @import url('https://fonts.googleapis.com/css2?family=Noto+Naskh+Arabic:wght@400;700&display=swap');
    
    .main {
        background: linear-gradient(135deg, #1e3c72 0%, #2a5298 100%);
        font-family: 'Noto Naskh Arabic', serif;
    }
    
    .stApp {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
    }
    
    .css-1d391kg {
        background: rgba(255, 255, 255, 0.1);
        backdrop-filter: blur(10px);
        border-radius: 15px;
        padding: 20px;
    }
    
    .stButton>button {
        background: linear-gradient(45deg, #f093fb 0%, #f5576c 100%);
        color: white;
        border: none;
        border-radius: 25px;
        padding: 15px 32px;
        font-size: 18px;
        font-weight: bold;
        transition: all 0.3s ease;
        box-shadow: 0 4px 15px rgba(0,0,0,0.2);
    }
    
    .stButton>button:hover {
        transform: translateY(-2px);
        box-shadow: 0 6px 20px rgba(0,0,0,0.3);
    }
    
    .metric-card {
        background: rgba(255, 255, 255, 0.15);
        backdrop-filter: blur(10px);
        border-radius: 15px;
        padding: 20px;
        border: 1px solid rgba(255, 255, 255, 0.2);
        margin: 10px 0;
    }
    
    .poem-text {
        font-size: 24px;
        line-height: 2;
        text-align: center;
        color: #ffffff;
        background: rgba(0, 0, 0, 0.3);
        padding: 30px;
        border-radius: 15px;
        border-right: 5px solid #f5576c;
        margin: 20px 0;
        font-family: 'Noto Naskh Arabic', serif;
    }
    
    .analysis-result {
        background: rgba(255, 255, 255, 0.1);
        backdrop-filter: blur(10px);
        border-radius: 15px;
        padding: 25px;
        margin: 15px 0;
        border: 1px solid rgba(255, 255, 255, 0.2);
    }
    
    .tafeela-box {
        display: inline-block;
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        padding: 10px 20px;
        margin: 5px;
        border-radius: 25px;
        font-weight: bold;
        box-shadow: 0 4px 15px rgba(0,0,0,0.2);
    }
    
    .fault-badge {
        display: inline-block;
        padding: 5px 15px;
        margin: 3px;
        border-radius: 20px;
        font-size: 14px;
        font-weight: bold;
    }
    
    .fault-acceptable {
        background: #ffd93d;
        color: #333;
    }
    
    .fault-critical {
        background: #ff6b6b;
        color: white;
    }
    
    .success-box {
        background: linear-gradient(135deg, #11998e 0%, #38ef7d 100%);
        color: white;
        padding: 20px;
        border-radius: 15px;
        text-align: center;
        font-size: 20px;
        font-weight: bold;
        margin: 20px 0;
    }
    
    .header-title {
        font-size: 48px;
        font-weight: bold;
        text-align: center;
        color: #ffffff;
        text-shadow: 2px 2px 4px rgba(0,0,0,0.3);
        margin-bottom: 10px;
        font-family: 'Noto Naskh Arabic', serif;
    }
    
    .subtitle {
        font-size: 20px;
        text-align: center;
        color: rgba(255,255,255,0.9);
        margin-bottom: 30px;
    }
    
    .sidebar-title {
        font-size: 24px;
        font-weight: bold;
        color: #ffffff;
        margin-bottom: 20px;
        text-align: center;
    }
    
    .info-box {
        background: rgba(255, 255, 255, 0.1);
        border-left: 4px solid #f5576c;
        padding: 15px;
        border-radius: 5px;
        margin: 10px 0;
    }
</style>
""", unsafe_allow_html=True)

# â•â•â• Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª â•â•â•
@dataclass
class ShatrAnalysis:
    original_text: str = ""
    arudi_text: str = ""
    binary_code: str = ""
    tafeelat: List[Dict] = field(default_factory=list)
    meter_name: Optional[str] = None
    meter_type: Optional[str] = None
    faults: List[Dict] = field(default_factory=list)
    confidence: float = 0.0
    is_valid: bool = False
    suggested_correction: Optional[str] = None

@dataclass
class PoemAnalysis:
    verses: List[ShatrAnalysis] = field(default_factory=list)
    unified_meter: Optional[str] = None
    meter_type: Optional[str] = None
    overall_confidence: float = 0.0
    is_monorhyme: bool = True

# â•â•â• Ù…Ø­Ø±Ùƒ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ù…ØªÙ‚Ø¯Ù… â•â•â•
class ArabicTextEngine:
    """Ù…Ø­Ø±Ùƒ Ù…ØªÙ‚Ø¯Ù… Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©"""
    
    ARABIC_LETTERS = {
        'Ø§': {'name': 'Ø£Ù„Ù', 'is_vowel': True, 'is_long': True},
        'Ø¨': {'name': 'Ø¨Ø§Ø¡'}, 'Øª': {'name': 'ØªØ§Ø¡'}, 'Ø«': {'name': 'Ø«Ø§Ø¡'},
        'Ø¬': {'name': 'Ø¬ÙŠÙ…'}, 'Ø­': {'name': 'Ø­Ø§Ø¡'}, 'Ø®': {'name': 'Ø®Ø§Ø¡'},
        'Ø¯': {'name': 'Ø¯Ø§Ù„'}, 'Ø°': {'name': 'Ø°Ø§Ù„'},
        'Ø±': {'name': 'Ø±Ø§Ø¡', 'is_qalqala': True},
        'Ø²': {'name': 'Ø²Ø§ÙŠ'}, 'Ø³': {'name': 'Ø³ÙŠÙ†'}, 'Ø´': {'name': 'Ø´ÙŠÙ†'},
        'Øµ': {'name': 'ØµØ§Ø¯'},
        'Ø¶': {'name': 'Ø¶Ø§Ø¯', 'is_qalqala': True},
        'Ø·': {'name': 'Ø·Ø§Ø¡', 'is_qalqala': True},
        'Ø¸': {'name': 'Ø¸Ø§Ø¡'}, 'Ø¹': {'name': 'Ø¹ÙŠÙ†'}, 'Øº': {'name': 'ØºÙŠÙ†'},
        'Ù': {'name': 'ÙØ§Ø¡'},
        'Ù‚': {'name': 'Ù‚Ø§Ù', 'is_qalqala': True},
        'Ùƒ': {'name': 'ÙƒØ§Ù'}, 'Ù„': {'name': 'Ù„Ø§Ù…'}, 'Ù…': {'name': 'Ù…ÙŠÙ…'},
        'Ù†': {'name': 'Ù†ÙˆÙ†'}, 'Ù‡': {'name': 'Ù‡Ø§Ø¡'},
        'Ùˆ': {'name': 'ÙˆØ§Ùˆ', 'is_vowel': True, 'is_long': True},
        'ÙŠ': {'name': 'ÙŠØ§Ø¡', 'is_vowel': True, 'is_long': True},
        'Ù‰': {'name': 'Ø£Ù„Ù Ù…Ù‚ØµÙˆØ±Ø©', 'is_vowel': True, 'is_long': True},
        'Ø©': {'name': 'ØªØ§Ø¡ Ù…Ø±Ø¨ÙˆØ·Ø©'}, 'Ø¡': {'name': 'Ù‡Ù…Ø²Ø©'},
        'Ø¤': {'name': 'Ù‡Ù…Ø²Ø© Ø¹Ù„Ù‰ ÙˆØ§Ùˆ'}, 'Ø¦': {'name': 'Ù‡Ù…Ø²Ø© Ø¹Ù„Ù‰ ÙŠØ§Ø¡'},
        'Ø¥': {'name': 'Ø£Ù„Ù Ù‡Ù…Ø²Ø© ØªØ­Øª', 'is_vowel': True},
        'Ø£': {'name': 'Ø£Ù„Ù Ù‡Ù…Ø²Ø© ÙÙˆÙ‚', 'is_vowel': True},
        'Ø¢': {'name': 'Ø£Ù„Ù Ù…Ø¯', 'is_vowel': True, 'is_long': True},
    }
    
    HARAKAT = {
        'Ù': {'name': 'ÙØªØ­Ø©', 'type': 'mutaharrik', 'weight': 1},
        'Ù': {'name': 'Ø¶Ù…Ø©', 'type': 'mutaharrik', 'weight': 1},
        'Ù': {'name': 'ÙƒØ³Ø±Ø©', 'type': 'mutaharrik', 'weight': 1},
        'Ù’': {'name': 'Ø³ÙƒÙˆÙ†', 'type': 'sakin', 'weight': 0},
        'Ù‘': {'name': 'Ø´Ø¯Ø©', 'type': 'shadda', 'weight': 2},
        'Ù‹': {'name': 'ØªÙ†ÙˆÙŠÙ† ÙØªØ­', 'type': 'tanween', 'weight': 2},
        'ÙŒ': {'name': 'ØªÙ†ÙˆÙŠÙ† Ø¶Ù…', 'type': 'tanween', 'weight': 2},
        'Ù': {'name': 'ØªÙ†ÙˆÙŠÙ† ÙƒØ³Ø±', 'type': 'tanween', 'weight': 2},
        'Ù°': {'name': 'Ø£Ù„Ù Ù…Ø¯ Ø¹Ù„ÙŠØ©', 'type': 'long_vowel', 'weight': 0},
        'Ù“': {'name': 'Ù…Ø¯', 'type': 'madd', 'weight': 0},
    }

    @classmethod
    def normalize_text(cls, text: str) -> str:
        """ØªØ·Ø¨ÙŠØ¹ Ø´Ø§Ù…Ù„ Ù„Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ"""
        if not text:
            return ""
        
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªØ·ÙˆÙŠÙ„
        text = text.replace('\u0640', '')
        
        # ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ù‡Ù…Ø²Ø§Øª
        hamza_map = {'Ø£': 'Ø§', 'Ø¥': 'Ø§', 'Ø¢': 'Ø§', 'Ù±': 'Ø§', 'Ø¤': 'Ùˆ', 'Ø¦': 'ÙŠ'}
        for old, new in hamza_map.items():
            text = text.replace(old, new)
        
        # ØªÙˆØ­ÙŠØ¯ Ø§Ù„ØªØ§Ø¡Ø§Øª
        text = text.replace('Ø©', 'Ù‡')
        
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø£Ø­Ø±Ù ØºÙŠØ± Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
        allowed = set(cls.ARABIC_LETTERS.keys()) | set(cls.HARAKAT.keys()) | {' ', '\n', '\t', 'ØŒ', '.', 'Ø›', 'ØŸ'}
        text = ''.join(c for c in text if c in allowed)
        
        return ' '.join(text.split())

    @classmethod
    def tokenize_phonetic(cls, text: str) -> List[Dict]:
        """ØªÙ‚Ø·ÙŠØ¹ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙˆØ­Ø¯Ø§Øª ØµÙˆØªÙŠØ© (Ø­Ø±Ù + Ø­Ø±ÙƒØ©)"""
        text = cls.normalize_text(text)
        tokens = []
        i = 0
        
        while i < len(text):
            char = text[i]
            
            if char in ' \t\nØŒ.Ø›ØŸ':
                i += 1
                continue
            
            if char in cls.ARABIC_LETTERS:
                letter_info = cls.ARABIC_LETTERS[char].copy()
                haraka = None
                next_idx = i + 1
                
                if next_idx < len(text) and text[next_idx] in cls.HARAKAT:
                    haraka_symbol = text[next_idx]
                    haraka = cls.HARAKAT[haraka_symbol].copy()
                    haraka['symbol'] = haraka_symbol
                    
                    # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø´Ø¯Ø© = Ø­Ø±ÙØ§Ù† (Ø³Ø§ÙƒÙ† + Ù…ØªØ­Ø±Ùƒ)
                    if haraka['type'] == 'shadda':
                        tokens.append({
                            'letter': char,
                            'letter_info': letter_info,
                            'haraka': {'name': 'Ø³ÙƒÙˆÙ† Ø¶Ù…Ù†ÙŠ', 'type': 'sakin', 'symbol': 'Ù’'},
                            'is_shadda_first': True
                        })
                        
                        if next_idx + 1 < len(text) and text[next_idx + 1] in cls.HARAKAT:
                            real_haraka = cls.HARAKAT[text[next_idx + 1]].copy()
                            real_haraka['symbol'] = text[next_idx + 1]
                            tokens.append({
                                'letter': char,
                                'letter_info': letter_info,
                                'haraka': real_haraka,
                                'is_shadda_second': True
                            })
                            i += 3
                        else:
                            tokens.append({
                                'letter': char,
                                'letter_info': letter_info,
                                'haraka': {'name': 'Ø³ÙƒÙˆÙ†', 'type': 'sakin', 'symbol': 'Ù’'},
                                'is_shadda_second': True
                            })
                            i += 2
                        continue
                    
                    # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØªÙ†ÙˆÙŠÙ† = Ø­Ø±ÙƒØ© + Ù†ÙˆÙ† Ø³Ø§ÙƒÙ†Ø©
                    elif haraka['type'] == 'tanween':
                        tokens.append({
                            'letter': char,
                            'letter_info': letter_info,
                            'haraka': haraka,
                            'has_tanween': True
                        })
                        
                        noon_info = cls.ARABIC_LETTERS['Ù†'].copy()
                        tokens.append({
                            'letter': 'Ù†',
                            'letter_info': noon_info,
                            'haraka': {'name': 'Ø³ÙƒÙˆÙ†', 'type': 'sakin', 'symbol': 'Ù’'},
                            'is_tanween_noon': True
                        })
                        i += 2
                        continue
                    
                    i += 2
                else:
                    # Ù„Ø§ Ø­Ø±ÙƒØ© = Ø³ÙƒÙˆÙ† Ø§ÙØªØ±Ø§Ø¶ÙŠ
                    haraka = {'name': 'Ø³ÙƒÙˆÙ† Ø§ÙØªØ±Ø§Ø¶ÙŠ', 'type': 'sakin', 'symbol': 'Ù’'}
                    i += 1
                
                tokens.append({
                    'letter': char,
                    'letter_info': letter_info,
                    'haraka': haraka
                })
            else:
                i += 1
        
        return tokens

    @classmethod
    def tokens_to_arudi(cls, tokens: List[Dict]) -> str:
        """ØªØ­ÙˆÙŠÙ„ Ø§Ù„ÙˆØ­Ø¯Ø§Øª Ø§Ù„ØµÙˆØªÙŠØ© Ø¥Ù„Ù‰ Ù†Øµ Ø¹Ø±ÙˆØ¶ÙŠ"""
        arudi_parts = []
        
        for token in tokens:
            letter = token['letter']
            haraka_type = token['haraka']['type']
            
            if haraka_type == 'mutaharrik':
                symbol = 'Ù…'  # Ù…ØªØ­Ø±Ùƒ
            elif haraka_type == 'sakin':
                symbol = 'Ø³'  # Ø³Ø§ÙƒÙ†
            elif haraka_type == 'long_vowel':
                symbol = 'Ø·'  # Ø·ÙˆÙŠÙ„ (Ù…Ø¯)
            else:
                symbol = 'Ø³'  # Ø§ÙØªØ±Ø§Ø¶ÙŠ Ø³Ø§ÙƒÙ†
            
            arudi_parts.append(f"{letter}{symbol}")
        
        return ' '.join(arudi_parts)

    @classmethod
    def tokens_to_binary(cls, tokens: List[Dict]) -> str:
        """ØªØ­ÙˆÙŠÙ„ Ø§Ù„ÙˆØ­Ø¯Ø§Øª Ø§Ù„ØµÙˆØªÙŠØ© Ø¥Ù„Ù‰ Ù†Ù…Ø· Ø«Ù†Ø§Ø¦ÙŠ"""
        binary = []
        
        for token in tokens:
            haraka_type = token['haraka']['type']
            
            # Ù‚Ø§Ø¹Ø¯Ø©: Ø§Ù„Ù…ØªØ­Ø±Ùƒ = 1ØŒ Ø§Ù„Ø³Ø§ÙƒÙ† = 0
            if haraka_type == 'mutaharrik':
                binary.append('1')
            elif haraka_type == 'sakin':
                binary.append('0')
            elif haraka_type == 'long_vowel':
                binary.append('0')
            else:
                binary.append('1')
        
        return ''.join(binary)

# â•â•â• Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¨Ø­ÙˆØ± Ø§Ù„ÙƒØ§Ù…Ù„Ø© â•â•â•
class MetersDatabase:
    """Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø´Ø§Ù…Ù„Ø© Ù„Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¨Ø­ÙˆØ± ÙˆØ£Ù†ÙˆØ§Ø¹Ù‡Ø§"""
    
    TAFEELAT = {
        'ÙØ¹ÙˆÙ„Ù†': {'binary': '11010', 'feet': 2, 'sabab': 'Ø®Ø¨Ù†', 'watad': 'Ù…Ø¬Ø±ÙˆØ±'},
        'Ù…ÙØ§Ø¹ÙŠÙ„Ù†': {'binary': '1101010', 'feet': 3, 'sabab': 'ÙˆØªØ±', 'watad': 'Ù…Ø¬Ø±ÙˆØ±'},
        'Ù…ÙØ§Ø¹Ù„Ù†': {'binary': '110110', 'feet': 2, 'sabab': 'Ø®Ø¨Ù†', 'watad': 'Ù…Ø¬Ø±ÙˆØ±'},
        'ÙØ§Ø¹Ù„Ø§ØªÙ†': {'binary': '1011010', 'feet': 3, 'sabab': 'ÙˆØªØ±', 'watad': 'Ù…Ø¬Ø±ÙˆØ±'},
        'ÙØ§Ø¹Ù„Ù†': {'binary': '10110', 'feet': 2, 'sabab': 'Ø®Ø¨Ù†', 'watad': 'Ù…Ø¬Ø±ÙˆØ±'},
        'Ù…Ø³ØªÙØ¹Ù„Ù†': {'binary': '1011010', 'feet': 3, 'sabab': 'ÙˆØªØ±', 'watad': 'Ù…Ø¬Ø±ÙˆØ±'},
        'Ù…ØªÙØ§Ø¹Ù„Ù†': {'binary': '1110110', 'feet': 3, 'sabab': 'ÙˆØªØ±', 'watad': 'Ù…Ø¬Ø±ÙˆØ±'},
        'Ù…ÙØ§Ø¹Ù„ØªÙ†': {'binary': '1101110', 'feet': 3, 'sabab': 'ÙˆØªØ±', 'watad': 'Ù…Ø¬Ø±ÙˆØ±'},
        'ÙØ¹ÙˆÙ„': {'binary': '1101', 'feet': 1.5, 'sabab': 'Ø®Ø¨Ù†', 'incomplete': True},
        'ÙØ§Ø¹Ù„': {'binary': '101', 'feet': 1.5, 'sabab': 'Ø®Ø¨Ù†', 'incomplete': True},
        'Ù…ÙØ§Ø¹': {'binary': '110', 'feet': 1.5, 'sabab': 'Ø®Ø¨Ù†', 'incomplete': True},
        'Ù…Ø³ØªÙØ¹Ù„': {'binary': '10110', 'feet': 2, 'sabab': 'Ø®Ø¨Ù†', 'incomplete': True},
    }
    
    ZAHAFAAT = {
        'ÙØ¹ÙˆÙ„Ù†': [
            {'pattern': '11010', 'name': 'Ø§Ù„Ø£ØµÙ„', 'valid': True},
            {'pattern': '1100', 'name': 'Ø®Ø¨Ù†', 'valid': True, 'fault': 'Ø®Ø¨Ù†', 'desc': 'Ø­Ø°Ù Ø§Ù„Ø³Ø§ÙƒÙ† Ø§Ù„Ø®Ø§Ù…Ø³'},
            {'pattern': '1110', 'name': 'Ø·ÙŠ', 'valid': True, 'fault': 'Ø·ÙŠ', 'desc': 'Ù†Ù‚Ù„ Ø§Ù„Ø­Ø±ÙƒØ© Ø¥Ù„Ù‰ Ø§Ù„Ø³Ø§ÙƒÙ†'},
            {'pattern': '11011', 'name': 'Ø¥Ø¹Ù„Ø§Ù„', 'valid': True, 'fault': 'Ø¥Ø¹Ù„Ø§Ù„', 'desc': 'ØªØºÙŠÙŠØ± Ø§Ù„Ø­Ø±ÙƒØ©'},
        ],
        'Ù…ÙØ§Ø¹ÙŠÙ„Ù†': [
            {'pattern': '1101010', 'name': 'Ø§Ù„Ø£ØµÙ„', 'valid': True},
            {'pattern': '110100', 'name': 'Ø®Ø¨Ù†', 'valid': True, 'fault': 'Ø®Ø¨Ù†', 'desc': 'Ø­Ø°Ù Ø§Ù„Ù†ÙˆÙ† Ø§Ù„Ø³Ø§ÙƒÙ†Ø©'},
            {'pattern': '110110', 'name': 'Ø¥Ù‚Ø§Ù…Ø©', 'valid': True, 'fault': 'Ø¥Ù‚Ø§Ù…Ø©', 'desc': 'Ù‚Ù„Ø¨ Ø§Ù„ÙˆØªØ±'},
            {'pattern': '1101011', 'name': 'ÙƒØ³Ø±', 'valid': True, 'fault': 'ÙƒØ³Ø±', 'desc': 'Ù†Ù‚Ù„ Ø§Ù„Ø­Ø±ÙƒØ© ÙÙŠ Ø§Ù„ÙˆØªØ±'},
        ],
        'ÙØ§Ø¹Ù„Ø§ØªÙ†': [
            {'pattern': '1011010', 'name': 'Ø§Ù„Ø£ØµÙ„', 'valid': True},
            {'pattern': '101100', 'name': 'Ø®Ø¨Ù†', 'valid': True, 'fault': 'Ø®Ø¨Ù†'},
            {'pattern': '101110', 'name': 'Ø·ÙŠ', 'valid': True, 'fault': 'Ø·ÙŠ'},
            {'pattern': '101111', 'name': 'Ø¥Ø¹Ù„Ø§Ù„', 'valid': True, 'fault': 'Ø¥Ø¹Ù„Ø§Ù„'},
        ],
        'Ù…Ø³ØªÙØ¹Ù„Ù†': [
            {'pattern': '1011010', 'name': 'Ø§Ù„Ø£ØµÙ„', 'valid': True},
            {'pattern': '101100', 'name': 'Ø®Ø¨Ù†', 'valid': True, 'fault': 'Ø®Ø¨Ù†'},
            {'pattern': '11010', 'name': 'ØªØ³Ù‡ÙŠÙ„', 'valid': True, 'fault': 'ØªØ³Ù‡ÙŠÙ„', 'desc': 'ØªØ®ÙÙŠÙ Ø§Ù„Ù…Ø³ØªÙØ¹Ù„Ù† Ø¥Ù„Ù‰ ÙØ¹ÙˆÙ„Ù†'},
            {'pattern': '1010110', 'name': 'Ø¥Ø¨Ø¯Ø§Ù„', 'valid': True, 'fault': 'Ø¥Ø¨Ø¯Ø§Ù„', 'desc': 'ØªØ¨Ø¯ÙŠÙ„ Ø§Ù„Ø³Ø§ÙƒÙ†ÙŠÙ†'},
        ],
        'Ù…ØªÙØ§Ø¹Ù„Ù†': [
            {'pattern': '1110110', 'name': 'Ø§Ù„Ø£ØµÙ„', 'valid': True},
            {'pattern': '111010', 'name': 'Ø®Ø¨Ù†', 'valid': True, 'fault': 'Ø®Ø¨Ù†'},
            {'pattern': '1111110', 'name': 'Ø·ÙŠ', 'valid': True, 'fault': 'Ø·ÙŠ'},
        ],
    }
    
    METERS = {
        'Ø§Ù„Ø·ÙˆÙŠÙ„': {
            'tafeelat': ['ÙØ¹ÙˆÙ„Ù†', 'Ù…ÙØ§Ø¹ÙŠÙ„Ù†', 'ÙØ¹ÙˆÙ„Ù†', 'Ù…ÙØ§Ø¹Ù„Ù†'],
            'base_pattern': '11010110101011010110101',
            'types': {
                'ØªØ§Ù…': {'tafeelat': ['ÙØ¹ÙˆÙ„Ù†', 'Ù…ÙØ§Ø¹ÙŠÙ„Ù†', 'ÙØ¹ÙˆÙ„Ù†', 'Ù…ÙØ§Ø¹Ù„Ù†'], 'cut': None, 'desc': 'Ø£Ø±Ø¨Ø¹ ØªÙØ¹ÙŠÙ„Ø§Øª ÙƒØ§Ù…Ù„Ø©'},
                'Ù…Ø¬Ø²ÙˆØ¡': {'tafeelat': ['ÙØ¹ÙˆÙ„Ù†', 'Ù…ÙØ§Ø¹ÙŠÙ„Ù†', 'ÙØ¹ÙˆÙ„Ù†'], 'cut': 'Ø§Ù„Ø¹Ø±ÙˆØ¶', 'desc': 'Ø­Ø°Ù Ø¢Ø®Ø± ØªÙØ¹ÙŠÙ„Ø©'},
                'Ù…Ø´Ø·ÙˆØ±': {'tafeelat': ['ÙØ¹ÙˆÙ„Ù†', 'Ù…ÙØ§Ø¹ÙŠÙ„Ù†'], 'cut': 'Ø§Ù„Ø¶Ø±Ø¨', 'desc': 'Ø­Ø°Ù Ø¢Ø®Ø± ØªÙØ¹ÙŠÙ„ØªÙŠÙ†'},
                'Ù…Ù†Ù‡ÙˆÙƒ': {'tafeelat': ['ÙØ¹ÙˆÙ„Ù†', 'Ù…ÙØ§Ø¹ÙŠÙ„Ù†', 'ÙØ¹ÙˆÙ„'], 'cut': 'Ø§Ù„Ø¹Ø±ÙˆØ¶ Ù…Ù‚Ø·ÙˆØ¹', 'desc': 'Ù…Ù‚Ø·ÙˆØ¹ Ø§Ù„Ø¹Ø±ÙˆØ¶'},
            },
            'description': 'Ø£Ø·ÙˆÙ„ Ø§Ù„Ø¨Ø­ÙˆØ±ØŒ ÙŠÙ…ØªØ§Ø² Ø¨Ø«Ù‚Ù„ Ø¥ÙŠÙ‚Ø§Ø¹Ù‡',
            'origin': 'Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠ'
        },
        'Ø§Ù„Ù…Ø¯ÙŠØ¯': {
            'tafeelat': ['ÙØ§Ø¹Ù„Ø§ØªÙ†', 'ÙØ§Ø¹Ù„Ù†', 'ÙØ§Ø¹Ù„Ø§ØªÙ†'],
            'base_pattern': '1011010101101011010',
            'types': {
                'ØªØ§Ù…': {'tafeelat': ['ÙØ§Ø¹Ù„Ø§ØªÙ†', 'ÙØ§Ø¹Ù„Ù†', 'ÙØ§Ø¹Ù„Ø§ØªÙ†'], 'cut': None},
                'Ù…Ø¬Ø²ÙˆØ¡': {'tafeelat': ['ÙØ§Ø¹Ù„Ø§ØªÙ†', 'ÙØ§Ø¹Ù„Ù†'], 'cut': 'Ø§Ù„Ø¹Ø±ÙˆØ¶'},
                'Ù…Ø´Ø·ÙˆØ±': {'tafeelat': ['ÙØ§Ø¹Ù„Ø§ØªÙ†', 'ÙØ§Ø¹Ù„'], 'cut': 'Ø§Ù„Ø¶Ø±Ø¨'},
            },
            'description': 'Ø¨Ø­Ø± Ø§Ù„Ù…Ø¯ÙŠØ­ ÙˆØ§Ù„Ø£ØºØ±Ø§Ø¶ Ø§Ù„Ø·ÙˆÙŠÙ„Ø©',
            'origin': 'Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠ'
        },
        'Ø§Ù„Ø¨Ø³ÙŠØ·': {
            'tafeelat': ['Ù…Ø³ØªÙØ¹Ù„Ù†', 'ÙØ§Ø¹Ù„Ù†', 'Ù…Ø³ØªÙØ¹Ù„Ù†', 'ÙØ§Ø¹Ù„Ù†'],
            'base_pattern': '101101010110101101010110',
            'types': {
                'ØªØ§Ù…': {'tafeelat': ['Ù…Ø³ØªÙØ¹Ù„Ù†', 'ÙØ§Ø¹Ù„Ù†', 'Ù…Ø³ØªÙØ¹Ù„Ù†', 'ÙØ§Ø¹Ù„Ù†'], 'cut': None},
                'Ù…Ø¬Ø²ÙˆØ¡': {'tafeelat': ['Ù…Ø³ØªÙØ¹Ù„Ù†', 'ÙØ§Ø¹Ù„Ù†', 'Ù…Ø³ØªÙØ¹Ù„Ù†'], 'cut': 'Ø§Ù„Ø¹Ø±ÙˆØ¶'},
                'Ù…Ø´Ø·ÙˆØ±': {'tafeelat': ['Ù…Ø³ØªÙØ¹Ù„Ù†', 'ÙØ§Ø¹Ù„Ù†'], 'cut': 'Ø§Ù„Ø¶Ø±Ø¨'},
                'Ù…ØªØ¯Ø§Ø±Ùƒ': {'tafeelat': ['Ù…Ø³ØªÙØ¹Ù„', 'ÙØ§Ø¹Ù„Ù†', 'Ù…Ø³ØªÙØ¹Ù„', 'ÙØ§Ø¹Ù„Ù†'], 'cut': None, 'fault': 'ØªØ¯Ø§Ø±Ùƒ', 'desc': 'Ø²Ø­Ø§Ù Ø§Ù„ØªØ¯Ø§Ø±Ùƒ'},
            },
            'description': 'Ø£ÙƒØ«Ø± Ø§Ù„Ø¨Ø­ÙˆØ± Ø§Ø³ØªØ®Ø¯Ø§Ù…Ø§Ù‹ØŒ ÙŠÙØ³Ù…Ù‰ Ø¨Ø­Ø± Ø§Ù„Ø´Ø¹Ø± Ø§Ù„Ø¹Ø±Ø¨ÙŠ',
            'origin': 'Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠ'
        },
        'Ø§Ù„ÙˆØ§ÙØ±': {
            'tafeelat': ['Ù…ÙØ§Ø¹Ù„ØªÙ†', 'Ù…ÙØ§Ø¹Ù„ØªÙ†', 'ÙØ¹ÙˆÙ„Ù†'],
            'base_pattern': '1101110110111011010',
            'types': {
                'ØªØ§Ù…': {'tafeelat': ['Ù…ÙØ§Ø¹Ù„ØªÙ†', 'Ù…ÙØ§Ø¹Ù„ØªÙ†', 'ÙØ¹ÙˆÙ„Ù†'], 'cut': None},
                'Ù…Ø¬Ø²ÙˆØ¡': {'tafeelat': ['Ù…ÙØ§Ø¹Ù„ØªÙ†', 'Ù…ÙØ§Ø¹Ù„ØªÙ†'], 'cut': 'Ø§Ù„Ø¹Ø±ÙˆØ¶'},
            },
            'description': 'Ø¨Ø­Ø± Ø§Ù„ÙˆØµÙ ÙˆØ§Ù„ØºØ²Ù„',
            'origin': 'Ø§Ù„ØµÙ†Ø§Ø¹ÙŠ'
        },
        'Ø§Ù„ÙƒØ§Ù…Ù„': {
            'tafeelat': ['Ù…ØªÙØ§Ø¹Ù„Ù†', 'Ù…ØªÙØ§Ø¹Ù„Ù†', 'Ù…ØªÙØ§Ø¹Ù„Ù†'],
            'base_pattern': '111011011101101110110',
            'types': {
                'ØªØ§Ù…': {'tafeelat': ['Ù…ØªÙØ§Ø¹Ù„Ù†', 'Ù…ØªÙØ§Ø¹Ù„Ù†', 'Ù…ØªÙØ§Ø¹Ù„Ù†'], 'cut': None},
                'Ù…Ø¬Ø²ÙˆØ¡': {'tafeelat': ['Ù…ØªÙØ§Ø¹Ù„Ù†', 'Ù…ØªÙØ§Ø¹Ù„Ù†'], 'cut': 'Ø§Ù„Ø¹Ø±ÙˆØ¶'},
                'Ù…Ø´Ø·ÙˆØ±': {'tafeelat': ['Ù…ØªÙØ§Ø¹Ù„Ù†'], 'cut': 'Ø§Ù„Ø¶Ø±Ø¨'},
                'Ù…Ø¶Ø§Ø±Ø¹': {'tafeelat': ['Ù…ØªÙØ§Ø¹Ù„', 'Ù…ØªÙØ§Ø¹Ù„', 'Ù…ØªÙØ§Ø¹Ù„'], 'cut': None, 'fault': 'ØªØ¶Ø¹ÙŠÙ', 'desc': 'ØªØ¶Ø¹ÙŠÙ: Ø­Ø°Ù Ø§Ù„Ù†ÙˆÙ†'},
            },
            'description': 'Ø¨Ø­Ø± Ø§Ù„Ø³Ù‡ÙˆÙ„Ø© ÙˆØ§Ù„ØªØ¬Ø§Ù†Ø³',
            'origin': 'Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠ'
        },
        'Ø§Ù„Ù‡Ø²Ø¬': {
            'tafeelat': ['Ù…ÙØ§Ø¹ÙŠÙ„Ù†', 'ÙØ§Ø¹Ù„Ø§ØªÙ†'],
            'base_pattern': '11010101101010',
            'types': {
                'ØªØ§Ù…': {'tafeelat': ['Ù…ÙØ§Ø¹ÙŠÙ„Ù†', 'ÙØ§Ø¹Ù„Ø§ØªÙ†'], 'cut': None},
                'Ù…Ø¬Ø²ÙˆØ¡': {'tafeelat': ['Ù…ÙØ§Ø¹ÙŠÙ„Ù†'], 'cut': 'Ø§Ù„Ø¹Ø±ÙˆØ¶'},
                'Ù…Ø´Ø·ÙˆØ±': {'tafeelat': ['Ù…ÙØ§Ø¹ÙŠÙ„'], 'cut': 'Ø§Ù„Ø¶Ø±Ø¨'},
            },
            'description': 'Ø¨Ø­Ø± Ø§Ù„Ø®ÙØ© ÙˆØ§Ù„Ø³Ø±Ø¹Ø©',
            'origin': 'Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠ'
        },
        'Ø§Ù„Ø±Ø¬Ø²': {
            'tafeelat': ['Ù…Ø³ØªÙØ¹Ù„Ù†', 'Ù…Ø³ØªÙØ¹Ù„Ù†', 'Ù…Ø³ØªÙØ¹Ù„Ù†'],
            'base_pattern': '101101010110101011010',
            'types': {
                'ØªØ§Ù…': {'tafeelat': ['Ù…Ø³ØªÙØ¹Ù„Ù†', 'Ù…Ø³ØªÙØ¹Ù„Ù†', 'Ù…Ø³ØªÙØ¹Ù„Ù†'], 'cut': None},
                'Ù…Ø¬Ø²ÙˆØ¡': {'tafeelat': ['Ù…Ø³ØªÙØ¹Ù„Ù†', 'Ù…Ø³ØªÙØ¹Ù„Ù†'], 'cut': 'Ø§Ù„Ø¹Ø±ÙˆØ¶'},
                'Ù…Ø´Ø·ÙˆØ±': {'tafeelat': ['Ù…Ø³ØªÙØ¹Ù„Ù†'], 'cut': 'Ø§Ù„Ø¶Ø±Ø¨'},
                'Ù…Ø±Ø¨Ø¹': {'tafeelat': ['Ù…Ø³ØªÙØ¹Ù„Ù†', 'Ù…Ø³ØªÙØ¹Ù„Ù†', 'Ù…Ø³ØªÙØ¹Ù„Ù†', 'Ù…Ø³ØªÙØ¹Ù„Ù†'], 'cut': None, 'desc': 'Ø£Ø±Ø¨Ø¹ ØªÙØ¹ÙŠÙ„Ø§Øª'},
            },
            'description': 'Ø¨Ø­Ø± Ø§Ù„Ù‚ØµØ§Ø¦Ø¯ Ø§Ù„Ù‚ØµÙŠØ±Ø© ÙˆØ§Ù„Ø­ÙƒÙ…Ø©',
            'origin': 'Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠ'
        },
        'Ø§Ù„Ø±Ù…Ù„': {
            'tafeelat': ['ÙØ§Ø¹Ù„Ø§ØªÙ†', 'ÙØ§Ø¹Ù„Ø§ØªÙ†', 'ÙØ§Ø¹Ù„Ø§ØªÙ†'],
            'base_pattern': '101101010110101011010',
            'types': {
                'ØªØ§Ù…': {'tafeelat': ['ÙØ§Ø¹Ù„Ø§ØªÙ†', 'ÙØ§Ø¹Ù„Ø§ØªÙ†', 'ÙØ§Ø¹Ù„Ø§ØªÙ†'], 'cut': None},
                'Ù…Ø¬Ø²ÙˆØ¡': {'tafeelat': ['ÙØ§Ø¹Ù„Ø§ØªÙ†', 'ÙØ§Ø¹Ù„Ø§ØªÙ†'], 'cut': 'Ø§Ù„Ø¹Ø±ÙˆØ¶'},
                'Ù…Ø´Ø·ÙˆØ±': {'tafeelat': ['ÙØ§Ø¹Ù„Ø§ØªÙ†'], 'cut': 'Ø§Ù„Ø¶Ø±Ø¨'},
            },
            'description': 'Ø¨Ø­Ø± Ø§Ù„Ø±Ø«Ø§Ø¡ ÙˆØ§Ù„Ø­Ø²Ù†',
            'origin': 'Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠ'
        },
        'Ø§Ù„Ø³Ø±ÙŠØ¹': {
            'tafeelat': ['Ù…Ø³ØªÙØ¹Ù„Ù†', 'Ù…Ø³ØªÙØ¹Ù„Ù†', 'ÙØ§Ø¹Ù„Ù†'],
            'base_pattern': '1011010101101010110',
            'types': {
                'ØªØ§Ù…': {'tafeelat': ['Ù…Ø³ØªÙØ¹Ù„Ù†', 'Ù…Ø³ØªÙØ¹Ù„Ù†', 'ÙØ§Ø¹Ù„Ù†'], 'cut': None},
                'Ù…Ø¬Ø²ÙˆØ¡': {'tafeelat': ['Ù…Ø³ØªÙØ¹Ù„Ù†', 'ÙØ§Ø¹Ù„Ù†'], 'cut': 'Ø§Ù„Ø¹Ø±ÙˆØ¶'},
            },
            'description': 'Ø¨Ø­Ø± Ø§Ù„Ø³Ø±Ø¹Ø© ÙˆØ§Ù„Ø®ÙØ©',
            'origin': 'Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠ'
        },
        'Ø§Ù„Ù…Ù†Ø³Ø±Ø­': {
            'tafeelat': ['Ù…Ø³ØªÙØ¹Ù„Ù†', 'ÙØ§Ø¹Ù„Ø§ØªÙ†', 'Ù…Ø³ØªÙØ¹Ù„Ù†', 'ÙØ§Ø¹Ù„Ù†'],
            'base_pattern': '10110101010101011010',
            'types': {
                'ØªØ§Ù…': {'tafeelat': ['Ù…Ø³ØªÙØ¹Ù„Ù†', 'ÙØ§Ø¹Ù„Ø§ØªÙ†', 'Ù…Ø³ØªÙØ¹Ù„Ù†', 'ÙØ§Ø¹Ù„Ù†'], 'cut': None},
                'Ù…Ø¬Ø²ÙˆØ¡': {'tafeelat': ['Ù…Ø³ØªÙØ¹Ù„Ù†', 'ÙØ§Ø¹Ù„Ø§ØªÙ†'], 'cut': 'Ø§Ù„Ø¹Ø±ÙˆØ¶'},
            },
            'description': 'Ø¨Ø­Ø± Ø§Ù„Ø³Ù‡ÙˆÙ„Ø© ÙˆØ§Ù„Ø§Ù†Ø³ÙŠØ§Ø¨ÙŠØ©',
            'origin': 'Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠ'
        },
        'Ø§Ù„Ø®ÙÙŠÙ': {
            'tafeelat': ['ÙØ§Ø¹Ù„Ø§ØªÙ†', 'Ù…Ø³ØªÙØ¹Ù„Ù†', 'ÙØ§Ø¹Ù„Ø§ØªÙ†'],
            'base_pattern': '101101010110101011010',
            'types': {
                'ØªØ§Ù…': {'tafeelat': ['ÙØ§Ø¹Ù„Ø§ØªÙ†', 'Ù…Ø³ØªÙØ¹Ù„Ù†', 'ÙØ§Ø¹Ù„Ø§ØªÙ†'], 'cut': None},
                'Ù…Ø¬Ø²ÙˆØ¡': {'tafeelat': ['ÙØ§Ø¹Ù„Ø§ØªÙ†', 'Ù…Ø³ØªÙØ¹Ù„Ù†'], 'cut': 'Ø§Ù„Ø¹Ø±ÙˆØ¶'},
            },
            'description': 'Ø¨Ø­Ø± Ø§Ù„Ø®ÙØ© ÙˆØ§Ù„Ù„ÙŠÙˆÙ†Ø©',
            'origin': 'Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠ'
        },
        'Ø§Ù„Ù…Ø¶Ø§Ø±Ø¹': {
            'tafeelat': ['Ù…ÙØ§Ø¹Ù„ØªÙ†', 'ÙØ§Ø¹Ù„Ø§ØªÙ†'],
            'base_pattern': '11010101011010',
            'types': {
                'ØªØ§Ù…': {'tafeelat': ['Ù…ÙØ§Ø¹Ù„ØªÙ†', 'ÙØ§Ø¹Ù„Ø§ØªÙ†'], 'cut': None},
                'Ù…Ø¬Ø²ÙˆØ¡': {'tafeelat': ['Ù…ÙØ§Ø¹Ù„ØªÙ†'], 'cut': 'Ø§Ù„Ø¹Ø±ÙˆØ¶'},
            },
            'description': 'Ø¨Ø­Ø± Ø§Ù„ØªØ¶Ø±Ø¹ ÙˆØ§Ù„Ø¯Ø¹Ø§Ø¡',
            'origin': 'Ø§Ù„ØµÙ†Ø§Ø¹ÙŠ'
        },
        'Ø§Ù„Ù…Ù‚ØªØ¶Ø¨': {
            'tafeelat': ['ÙØ§Ø¹Ù„Ø§ØªÙ†', 'Ù…Ø³ØªÙØ¹Ù„Ù†', 'ÙØ§Ø¹Ù„Ù†'],
            'base_pattern': '10101011011010',
            'types': {
                'ØªØ§Ù…': {'tafeelat': ['ÙØ§Ø¹Ù„Ø§ØªÙ†', 'Ù…Ø³ØªÙØ¹Ù„Ù†', 'ÙØ§Ø¹Ù„Ù†'], 'cut': None},
            },
            'description': 'Ø¨Ø­Ø± Ù…Ø®ØªØµØ± ÙˆÙ…Ù‚ØªØ¶Ø¨',
            'origin': 'Ø§Ù„ØµÙ†Ø§Ø¹ÙŠ'
        },
        'Ø§Ù„Ù…Ø¬ØªØ«': {
            'tafeelat': ['Ù…Ø³ØªÙØ¹Ù„Ù†', 'ÙØ§Ø¹Ù„Ø§ØªÙ†', 'ÙØ§Ø¹Ù„Ù†'],
            'base_pattern': '10110101011010',
            'types': {
                'ØªØ§Ù…': {'tafeelat': ['Ù…Ø³ØªÙØ¹Ù„Ù†', 'ÙØ§Ø¹Ù„Ø§ØªÙ†', 'ÙØ§Ø¹Ù„Ù†'], 'cut': None},
            },
            'description': 'Ø¨Ø­Ø± Ø§Ù„Ù…Ø¬ØªØ«Ø§Øª',
            'origin': 'Ø§Ù„ØµÙ†Ø§Ø¹ÙŠ'
        },
        'Ø§Ù„Ù…ØªÙ‚Ø§Ø±Ø¨': {
            'tafeelat': ['ÙØ¹ÙˆÙ„Ù†', 'ÙØ¹ÙˆÙ„Ù†', 'ÙØ¹ÙˆÙ„Ù†', 'ÙØ¹ÙˆÙ„Ù†'],
            'base_pattern': '11010110101101011010',
            'types': {
                'ØªØ§Ù…': {'tafeelat': ['ÙØ¹ÙˆÙ„Ù†', 'ÙØ¹ÙˆÙ„Ù†', 'ÙØ¹ÙˆÙ„Ù†', 'ÙØ¹ÙˆÙ„Ù†'], 'cut': None},
                'Ù…Ø¬Ø²ÙˆØ¡': {'tafeelat': ['ÙØ¹ÙˆÙ„Ù†', 'ÙØ¹ÙˆÙ„Ù†', 'ÙØ¹ÙˆÙ„Ù†'], 'cut': 'Ø§Ù„Ø¹Ø±ÙˆØ¶'},
                'Ù…Ø´Ø·ÙˆØ±': {'tafeelat': ['ÙØ¹ÙˆÙ„Ù†', 'ÙØ¹ÙˆÙ„Ù†'], 'cut': 'Ø§Ù„Ø¶Ø±Ø¨'},
            },
            'description': 'Ø¨Ø­Ø± Ø§Ù„ØªÙ‚Ø§Ø±Ø¨ ÙˆØ§Ù„ØªØ¬Ø§Ù†Ø³',
            'origin': 'Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠ'
        },
        'Ø§Ù„Ù…ØªØ¯Ø§Ø±Ùƒ': {
            'tafeelat': ['ÙØ§Ø¹Ù„Ù†', 'ÙØ§Ø¹Ù„Ù†', 'ÙØ§Ø¹Ù„Ù†', 'ÙØ§Ø¹Ù„Ù†'],
            'base_pattern': '10110101101011010110',
            'types': {
                'ØªØ§Ù…': {'tafeelat': ['ÙØ§Ø¹Ù„Ù†', 'ÙØ§Ø¹Ù„Ù†', 'ÙØ§Ø¹Ù„Ù†', 'ÙØ§Ø¹Ù„Ù†'], 'cut': None},
                'Ù…Ø¬Ø²ÙˆØ¡': {'tafeelat': ['ÙØ§Ø¹Ù„Ù†', 'ÙØ§Ø¹Ù„Ù†', 'ÙØ§Ø¹Ù„Ù†'], 'cut': 'Ø§Ù„Ø¹Ø±ÙˆØ¶'},
            },
            'description': 'Ø¨Ø­Ø± Ø§Ù„ØªØ¯Ø§Ø±Ùƒ ÙˆØ§Ù„Ø³Ø±Ø¹Ø© Ø§Ù„ÙØ§Ø¦Ù‚Ø©',
            'origin': 'Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠ'
        },
    }

# â•â•â• Ù…Ø­Ù„Ù„ Ø§Ù„Ø¹Ù„Ù„ Ø§Ù„Ø¹Ø±ÙˆØ¶ÙŠØ© â•â•â•
class FaultsAnalyzer:
    """Ù…Ø­Ù„Ù„ Ø§Ù„Ø¹Ù„Ù„ Ø§Ù„Ø¹Ø±ÙˆØ¶ÙŠØ©: Ø§Ù„Ø²Ø­Ø§ÙØ§Øª ÙˆØ§Ù„Ø¹Ù„Ù„"""
    
    @classmethod
    def detect_faults(cls, binary: str, expected_pattern: str, tafeela_name: str) -> List[Dict]:
        """ÙƒØ´Ù Ø§Ù„Ø¹Ù„Ù„ ÙÙŠ Ø§Ù„ØªÙØ¹ÙŠÙ„Ø©"""
        faults = []
        
        if len(binary) != len(expected_pattern):
            faults.append({
                'type': 'length_mismatch',
                'expected': len(expected_pattern),
                'actual': len(binary),
                'severity': 'critical'
            })
            return faults
        
        for i, (exp, act) in enumerate(zip(expected_pattern, binary)):
            if exp != act:
                fault_type = cls._classify_fault(i, exp, act, tafeela_name)
                faults.append({
                    'position': i,
                    'expected': exp,
                    'actual': act,
                    'type': fault_type['name'],
                    'description': fault_type['desc'],
                    'severity': fault_type.get('severity', 'minor'),
                    'can_correct': fault_type.get('can_correct', True)
                })
        
        return faults
    
    @classmethod
    def _classify_fault(cls, position: int, expected: str, actual: str, tafeela: str) -> Dict:
        """ØªØµÙ†ÙŠÙ Ù†ÙˆØ¹ Ø§Ù„Ø®Ù„Ù„"""
        
        if position == 4 and expected == '0' and actual == '1':
            return {
                'name': 'Ø®Ø¨Ù†',
                'desc': 'Ø­Ø°Ù Ø§Ù„Ø³Ø§ÙƒÙ† Ø§Ù„Ø®Ø§Ù…Ø³ (Ø§Ù„Ø®Ø¨Ù†)',
                'severity': 'acceptable',
                'can_correct': True
            }
        
        if expected == '0' and actual == '1':
            return {
                'name': 'Ø·ÙŠ',
                'desc': 'Ù†Ù‚Ù„ Ø§Ù„Ø­Ø±ÙƒØ© Ø¥Ù„Ù‰ Ø§Ù„Ø³Ø§ÙƒÙ† (Ø§Ù„Ø·ÙŠ)',
                'severity': 'acceptable',
                'can_correct': True
            }
        
        if tafeela == 'Ù…ÙØ§Ø¹ÙŠÙ„Ù†' and position in [4, 5]:
            return {
                'name': 'Ø¥Ù‚Ø§Ù…Ø©',
                'desc': 'Ù‚Ù„Ø¨ Ø§Ù„ÙˆØªØ± ÙÙŠ Ù…ÙØ§Ø¹ÙŠÙ„Ù†',
                'severity': 'acceptable',
                'can_correct': True
            }
        
        if tafeela == 'Ù…Ø³ØªÙØ¹Ù„Ù†' and len(expected) > len(actual):
            return {
                'name': 'ØªØ³Ù‡ÙŠÙ„',
                'desc': 'ØªØ®ÙÙŠÙ Ø§Ù„Ù…Ø³ØªÙØ¹Ù„Ù† Ø¥Ù„Ù‰ ÙØ¹ÙˆÙ„Ù†',
                'severity': 'acceptable',
                'can_correct': True
            }
        
        return {
            'name': 'Ø®Ù„Ù„',
            'desc': 'Ø²Ø­Ø§Ù ØºÙŠØ± Ù…Ø³Ù…ÙˆØ­ Ø¨Ù‡ ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù…ÙˆØ¶Ø¹',
            'severity': 'critical',
            'can_correct': False
        }
    
    @classmethod
    def validate_with_faults(cls, binary: str, tafeela_name: str) -> Dict:
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„ØªÙØ¹ÙŠÙ„Ø© Ù…Ø¹ Ø§Ù„Ø³Ù…Ø§Ø­ Ø¨Ø§Ù„Ø¹Ù„Ù„ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹Ø©"""
        tafeela_info = MetersDatabase.TAFEELAT.get(tafeela_name, {})
        expected = tafeela_info.get('binary', '')
        
        if not expected:
            return {'valid': False, 'reason': 'ØªÙØ¹ÙŠÙ„Ø© ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙØ©'}
        
        if binary == expected:
            return {
                'valid': True,
                'has_faults': False,
                'faults': [],
                'original_pattern': expected
            }
        
        allowed_patterns = MetersDatabase.ZAHAFAAT.get(tafeela_name, [])
        
        for variant in allowed_patterns:
            if binary == variant['pattern']:
                return {
                    'valid': True,
                    'has_faults': True,
                    'fault_name': variant['name'],
                    'fault_type': variant.get('fault'),
                    'fault_desc': variant.get('desc'),
                    'faults': [{
                        'type': variant.get('fault', 'Ø²Ø­Ø§Ù'),
                        'name': variant['name'],
                        'description': variant.get('desc', ''),
                        'acceptable': variant['valid']
                    }],
                    'original_pattern': expected,
                    'actual_pattern': binary
                }
        
        faults = cls.detect_faults(binary, expected, tafeela_name)
        
        return {
            'valid': False,
            'has_faults': True,
            'faults': faults,
            'original_pattern': expected,
            'actual_pattern': binary
        }

# â•â•â• Ø§Ù„ÙØ±Ø§Ù‡ÙŠØ¯ÙŠ Ø§Ù„Ø°ÙƒÙŠ Ø§Ù„Ù…ØªÙ‚Ø¯Ù… Pro â•â•â•
class FarahidiPro:
    """Ø§Ù„ÙØ±Ø§Ù‡ÙŠØ¯ÙŠ Ø§Ù„Ø°ÙƒÙŠ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…"""
    
    def __init__(self):
        self.text_engine = ArabicTextEngine()
        self.faults_analyzer = FaultsAnalyzer()
        self.meters_db = MetersDatabase()
    
    def analyze_shatr(self, text: str) -> ShatrAnalysis:
        """ØªØ­Ù„ÙŠÙ„ Ø´Ø·Ø± ÙˆØ§Ø­Ø¯ (ØµØ¯Ø± Ø£Ùˆ Ø¹Ø¬Ø²)"""
        
        # Ù¡. Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØªÙŠØ©
        tokens = self.text_engine.tokenize_phonetic(text)
        
        if not tokens:
            return ShatrAnalysis(
                original_text=text,
                is_valid=False,
                confidence=0.0
            )
        
        # Ù¢. Ø§Ù„ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Ø¹Ø±ÙˆØ¶ÙŠ ÙˆØ«Ù†Ø§Ø¦ÙŠ
        arudi_text = self.text_engine.tokens_to_arudi(tokens)
        binary_code = self.text_engine.tokens_to_binary(tokens)
        
        # Ù£. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØªÙØ¹ÙŠÙ„Ø§Øª Ù…Ø¹ Ø§Ù„Ø²Ø­Ø§ÙØ§Øª
        detected_tafeelat = self._extract_tafeelat(binary_code, tokens)
        
        # Ù¤. Ù…Ø·Ø§Ø¨Ù‚Ø© Ø¬Ù…ÙŠØ¹ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø¨Ø­ÙˆØ±
        meter_match = self._match_meter(binary_code, detected_tafeelat)
        
        # Ù¥. Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø¹Ù„Ù„
        faults = []
        for taf in detected_tafeelat:
            if taf.get('expected_name'):
                validation = self.faults_analyzer.validate_with_faults(
                    taf['binary'], taf['expected_name']
                )
                if validation.get('has_faults'):
                    faults.extend(validation.get('faults', []))
        
        # Ù¦. Ø­Ø³Ø§Ø¨ Ø§Ù„Ø«Ù‚Ø©
        confidence = self._calculate_confidence(detected_tafeelat, meter_match, faults)
        
        # Ù§. Ø§Ù„ØªØµØ­ÙŠØ­ Ø§Ù„Ù…Ù‚ØªØ±Ø­
        suggested_correction = None
        if faults and not all(f.get('severity') == 'critical' for f in faults):
            suggested_correction = self._generate_correction(text, faults)
        
        return ShatrAnalysis(
            original_text=text,
            arudi_text=arudi_text,
            binary_code=binary_code,
            tafeelat=detected_tafeelat,
            meter_name=meter_match.get('meter_name'),
            meter_type=meter_match.get('meter_type'),
            faults=faults,
            confidence=confidence,
            is_valid=meter_match.get('is_valid', False),
            suggested_correction=suggested_correction
        )
    
    def _extract_tafeelat(self, binary: str, tokens: List[Dict]) -> List[Dict]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØªÙØ¹ÙŠÙ„Ø§Øª Ù…Ù† Ø§Ù„Ù†Ù…Ø· Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù…Ø¹ Ø¯Ø¹Ù… Ø§Ù„Ø²Ø­Ø§ÙØ§Øª"""
        detected = []
        i = 0
        
        while i < len(binary):
            matched = False
            
            sorted_tafeelat = sorted(
                self.meters_db.TAFEELAT.items(),
                key=lambda x: len(x[1]['binary']),
                reverse=True
            )
            
            for tafeela_name, tafeela_info in sorted_tafeelat:
                pattern = tafeela_info['binary']
                pattern_len = len(pattern)
                
                if i + pattern_len <= len(binary):
                    segment = binary[i:i + pattern_len]
                    
                    if segment == pattern or self._is_valid_variation(segment, tafeela_name):
                        letter_count = self._count_letters_in_binary_segment(tokens, i, pattern_len)
                        
                        detected.append({
                            'name': tafeela_name,
                            'binary': segment,
                            'position': i,
                            'length': pattern_len,
                            'letters': letter_count,
                            'is_complete': not tafeela_info.get('incomplete', False),
                            'expected_name': tafeela_name
                        })
                        
                        i += pattern_len
                        matched = True
                        break
            
            if not matched:
                i += 1
        
        return detected
    
    def _is_valid_variation(self, binary: str, tafeela_name: str) -> bool:
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù†Ù…Ø· Ù…ØªØºÙŠØ±Ø§Ù‹ ØµØ­ÙŠØ­Ø§Ù‹ Ù„Ù„ØªÙØ¹ÙŠÙ„Ø©"""
        allowed = self.meters_db.ZAHAFAAT.get(tafeela_name, [])
        return any(variant['pattern'] == binary for variant in allowed)
    
    def _count_letters_in_binary_segment(self, tokens: List[Dict], start: int, length: int) -> List[str]:
        """Ø­Ø³Ø§Ø¨ Ø§Ù„Ø­Ø±ÙˆÙ Ø§Ù„Ù…Ù‚Ø§Ø¨Ù„Ø© Ù„Ø¬Ø²Ø¡ Ù…Ù† Ø§Ù„Ù†Ù…Ø· Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ"""
        letters = []
        current_pos = 0
        
        for token in tokens:
            if current_pos >= start and current_pos < start + length:
                letters.append(token['letter'])
            current_pos += 1
            
            if current_pos >= start + length:
                break
        
        return letters
    
    def _match_meter(self, binary: str, detected_tafeelat: List[Dict]) -> Dict:
        """Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„Ù†Ù…Ø· Ù…Ø¹ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¨Ø­ÙˆØ± (Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø£Ù†ÙˆØ§Ø¹)"""
        if not detected_tafeelat:
            return {'is_valid': False, 'meter_name': None, 'meter_type': None}
        
        detected_names = [t['name'] for t in detected_tafeelat]
        
        best_match = None
        best_score = 0
        
        for meter_name, meter_info in self.meters_db.METERS.items():
            for meter_type, type_info in meter_info.get('types', {}).items():
                expected_tafeelat = type_info['tafeelat']
                
                score = self._calculate_match_score(detected_names, expected_tafeelat)
                
                if score > best_score:
                    best_score = score
                    best_match = {
                        'meter_name': meter_name,
                        'meter_type': meter_type,
                        'score': score,
                        'is_valid': score >= 0.7,
                        'expected_tafeelat': expected_tafeelat,
                        'cut': type_info.get('cut')
                    }
        
        return best_match or {'is_valid': False, 'meter_name': None, 'meter_type': None}
    
    def _calculate_match_score(self, detected: List[str], expected: List[str]) -> float:
        """Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø© ØªØ·Ø§Ø¨Ù‚ Ø§Ù„ØªÙØ¹ÙŠÙ„Ø§Øª"""
        if not expected:
            return 0.0
        
        matches = 0
        total = max(len(detected), len(expected))
        
        for i, exp in enumerate(expected):
            if i < len(detected):
                if detected[i] == exp:
                    matches += 1
                elif self._is_valid_variation(detected[i], exp):
                    matches += 0.8
        
        return matches / total if total > 0 else 0.0
    
    def _calculate_confidence(self, tafeelat: List[Dict], meter_match: Dict, faults: List[Dict]) -> float:
        """Ø­Ø³Ø§Ø¨ Ù†Ø³Ø¨Ø© Ø§Ù„Ø«Ù‚Ø© ÙÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„"""
        base_confidence = meter_match.get('score', 0.0) * 100
        
        if faults:
            critical_count = sum(1 for f in faults if f.get('severity') == 'critical')
            acceptable_count = sum(1 for f in faults if f.get('severity') == 'acceptable')
            
            penalty = (critical_count * 20) + (acceptable_count * 5)
            base_confidence = max(0, base_confidence - penalty)
        
        return round(base_confidence, 2)
    
    def _generate_correction(self, text: str, faults: List[Dict]) -> Optional[str]:
        """ØªÙˆÙ„ÙŠØ¯ ØªØµØ­ÙŠØ­ Ù…Ù‚ØªØ±Ø­ Ù„Ù„Ù†Øµ"""
        if not faults:
            return None
        
        corrections = []
        for fault in faults:
            if fault.get('can_correct') and fault.get('severity') != 'critical':
                corrections.append(f"ØªØµØ­ÙŠØ­: {fault.get('description', '')}")
        
        return " | ".join(corrections) if corrections else None
    
    def analyze_full_poem(self, text: str) -> PoemAnalysis:
        """ØªØ­Ù„ÙŠÙ„ Ù‚ØµÙŠØ¯Ø© ÙƒØ§Ù…Ù„Ø©"""
        verses = text.strip().split('\n')
        analyses = []
        
        for verse in verses:
            if 'ØŒ' in verse or ' ' in verse:
                parts = verse.replace('ØŒ', ' ').split()
                mid = len(parts) // 2
                
                sadr = ' '.join(parts[:mid])
                ajuz = ' '.join(parts[mid:])
                
                sadr_analysis = self.analyze_shatr(sadr)
                ajuz_analysis = self.analyze_shatr(ajuz)
                
                analyses.extend([sadr_analysis, ajuz_analysis])
        
        unified_meter = self._determine_unified_meter(analyses)
        
        return PoemAnalysis(
            verses=analyses,
            unified_meter=unified_meter.get('name'),
            meter_type=unified_meter.get('type'),
            overall_confidence=unified_meter.get('confidence', 0.0)
        )
    
    def _determine_unified_meter(self, analyses: List[ShatrAnalysis]) -> Dict:
        """ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¨Ø­Ø± Ø§Ù„Ù…ÙˆØ­Ø¯ Ù„Ù„Ù‚ØµÙŠØ¯Ø©"""
        meter_votes = defaultdict(int)
        
        for analysis in analyses:
            if analysis.meter_name:
                key = f"{analysis.meter_name}_{analysis.meter_type}"
                meter_votes[key] += analysis.confidence
        
        if meter_votes:
            best_meter = max(meter_votes.items(), key=lambda x: x[1])
            meter_name, meter_type = best_meter[0].split('_')
            total_confidence = sum(a.confidence for a in analyses) / len(analyses) if analyses else 0
            
            return {
                'name': meter_name,
                'type': meter_type,
                'confidence': round(total_confidence, 2)
            }
        
        return {'name': None, 'type': None, 'confidence': 0.0}

# â•â•â• ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Streamlit â•â•â•
def render_header():
    """Ø¹Ø±Ø¶ Ø±Ø£Ø³ Ø§Ù„ØµÙØ­Ø©"""
    st.markdown('<h1 class="header-title">ğŸ“œ Ù…Ù†ØµØ© ØªØ§Ù… Ø§Ù„Ø«Ù‚Ø§ÙÙŠØ© Ø§Ù„Ø°ÙƒÙŠØ©</h1>', unsafe_allow_html=True)
    st.markdown('<p class="subtitle">Ù†Ø¸Ø§Ù… Ù…ØªÙ‚Ø¯Ù… Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø´Ø¹Ø± Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø§Ù„Ø¹Ø±ÙˆØ¶ÙŠ (Ø§Ù„ÙØ±Ø§Ù‡ÙŠØ¯ÙŠ)</p>', unsafe_allow_html=True)

def render_sidebar():
    """Ø¹Ø±Ø¶ Ø§Ù„Ø´Ø±ÙŠØ· Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠ"""
    with st.sidebar:
        st.markdown('<div class="sidebar-title">âš™ï¸ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª ÙˆØ§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª</div>', unsafe_allow_html=True)
        
        st.markdown("---")
        
        # Ø§Ø®ØªÙŠØ§Ø± Ù†ÙˆØ¹ Ø§Ù„ØªØ­Ù„ÙŠÙ„
        analysis_mode = st.radio(
            "Ù†ÙˆØ¹ Ø§Ù„ØªØ­Ù„ÙŠÙ„:",
            ["Ø´Ø·Ø± ÙˆØ§Ø­Ø¯", "Ø¨ÙŠØª ÙƒØ§Ù…Ù„", "Ù‚ØµÙŠØ¯Ø© ÙƒØ§Ù…Ù„Ø©"],
            index=0
        )
        
        st.markdown("---")
        
        # Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¹Ù† Ø§Ù„Ø¨Ø­ÙˆØ±
        with st.expander("ğŸ“š Ø§Ù„Ø¨Ø­ÙˆØ± Ø§Ù„Ø´Ø¹Ø±ÙŠØ© (16)"):
            for meter_name, info in MetersDatabase.METERS.items():
                st.markdown(f"""
                <div class="info-box">
                    <strong>{meter_name}</strong><br>
                    <small>{info['description']}</small><br>
                    <small>Ø§Ù„Ø£ØµÙ„: {info['origin']}</small>
                </div>
                """, unsafe_allow_html=True)
        
        # Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¹Ù† Ø§Ù„Ø²Ø­Ø§ÙØ§Øª
        with st.expander("ğŸ”§ Ø§Ù„Ø²Ø­Ø§ÙØ§Øª Ø§Ù„Ù…Ø³Ù…ÙˆØ­Ø©"):
            zahafat_info = """
            - **Ø®Ø¨Ù†**: Ø­Ø°Ù Ø§Ù„Ø³Ø§ÙƒÙ† Ø§Ù„Ø®Ø§Ù…Ø³
            - **Ø·ÙŠ**: Ù†Ù‚Ù„ Ø§Ù„Ø­Ø±ÙƒØ© Ø¥Ù„Ù‰ Ø§Ù„Ø³Ø§ÙƒÙ†
            - **Ø¥Ù‚Ø§Ù…Ø©**: Ù‚Ù„Ø¨ Ø§Ù„ÙˆØªØ± (ØªØ¨Ø¯ÙŠÙ„ Ø§Ù„Ø³Ø§ÙƒÙ†ÙŠÙ†)
            - **ØªØ³Ù‡ÙŠÙ„**: ØªØ®ÙÙŠÙ Ø§Ù„Ù…Ø³ØªÙØ¹Ù„Ù† Ø¥Ù„Ù‰ ÙØ¹ÙˆÙ„Ù†
            - **ÙƒØ³Ø±**: Ù†Ù‚Ù„ Ø§Ù„Ø­Ø±ÙƒØ© ÙÙŠ Ø§Ù„ÙˆØªØ±
            - **Ø¥Ø¹Ù„Ø§Ù„**: ØªØºÙŠÙŠØ± Ø§Ù„Ø­Ø±ÙƒØ©
            - **Ø¥Ø¨Ø¯Ø§Ù„**: ØªØ¨Ø¯ÙŠÙ„ Ø§Ù„Ø³Ø§ÙƒÙ†ÙŠÙ†
            """
            st.markdown(zahafat_info)
        
        st.markdown("---")
        st.markdown("""
        <div style="text-align: center; color: rgba(255,255,255,0.7); font-size: 12px;">
            Ù…Ù†ØµØ© ØªØ§Ù… Ø§Ù„Ø«Ù‚Ø§ÙÙŠØ© Ø§Ù„Ø°ÙƒÙŠØ© Â© 2025<br>
            Ø§Ù„Ø¥ØµØ¯Ø§Ø± 2.0 - Streamlit Edition
        </div>
        """, unsafe_allow_html=True)
        
        return analysis_mode

def render_analysis_input(analysis_mode: str):
    """Ø¹Ø±Ø¶ Ù…Ù†Ø·Ù‚Ø© Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„Ù†Øµ"""
    st.markdown("### âœï¸ Ø£Ø¯Ø®Ù„ Ø§Ù„Ù†Øµ Ø§Ù„Ø´Ø¹Ø±ÙŠ")
    
    if analysis_mode == "Ø´Ø·Ø± ÙˆØ§Ø­Ø¯":
        text = st.text_area(
            "Ø£Ø¯Ø®Ù„ Ø§Ù„Ø´Ø·Ø± (Ø§Ù„ØµØ¯Ø± Ø£Ùˆ Ø§Ù„Ø¹Ø¬Ø²):",
            height=100,
            placeholder="Ù…Ø«Ø§Ù„: ÙÙÙ„Ø§ ØªÙØ¸ÙÙ†ÙÙ‘Ù†ÙÙ‘ Ø£ÙÙ†ÙÙ‘ Ø§Ù„Ù„ÙÙ‘ÙŠØ«Ù ÙŠÙØ¨Ù’ØªÙØ³ÙÙ…Ù",
            help="Ø£Ø¯Ø®Ù„ Ø´Ø·Ø±Ø§Ù‹ ÙˆØ§Ø­Ø¯Ø§Ù‹ Ù…Ù† Ø§Ù„Ø¨ÙŠØª Ø§Ù„Ø´Ø¹Ø±ÙŠ Ù…Ø¹ Ø§Ù„ØªØ´ÙƒÙŠÙ„ Ø§Ù„ÙƒØ§Ù…Ù„"
        )
    elif analysis_mode == "Ø¨ÙŠØª ÙƒØ§Ù…Ù„":
        text = st.text_area(
            "Ø£Ø¯Ø®Ù„ Ø§Ù„Ø¨ÙŠØª Ø§Ù„ÙƒØ§Ù…Ù„:",
            height=100,
            placeholder="Ù…Ø«Ø§Ù„: ÙÙÙ„Ø§ ØªÙØ¸ÙÙ†ÙÙ‘Ù†ÙÙ‘ Ø£ÙÙ†ÙÙ‘ Ø§Ù„Ù„ÙÙ‘ÙŠØ«Ù ÙŠÙØ¨Ù’ØªÙØ³ÙÙ…Ù ØŒ ÙˆÙÙ…Ø§ Ø¨ÙØ±ÙØ¯Ù Ø§Ù„ØºÙÙŠØ«Ù Ø¨ÙØ§Ù„Ù…ÙØ±Ù’ØªÙØ¹Ù Ø§Ù„Ù†ÙÙ‘Ù…ÙÙ„Ù",
            help="Ø£Ø¯Ø®Ù„ Ø§Ù„Ø¨ÙŠØª Ø§Ù„Ø´Ø¹Ø±ÙŠ ÙƒØ§Ù…Ù„Ø§Ù‹ Ù…Ø¹ Ø¹Ù„Ø§Ù…Ø© Ø§Ù„ØªØ±Ù‚ÙŠÙ… Ø¨ÙŠÙ† Ø§Ù„ØµØ¯Ø± ÙˆØ§Ù„Ø¹Ø¬Ø²"
        )
    else:  # Ù‚ØµÙŠØ¯Ø© ÙƒØ§Ù…Ù„Ø©
        text = st.text_area(
            "Ø£Ø¯Ø®Ù„ Ø§Ù„Ù‚ØµÙŠØ¯Ø©:",
            height=300,
            placeholder="Ø£Ø¯Ø®Ù„ Ø§Ù„Ø£Ø¨ÙŠØ§Øª Ø§Ù„Ø´Ø¹Ø±ÙŠØ© ÙƒÙ„ Ø¨ÙŠØª ÙÙŠ Ø³Ø·Ø±...",
            help="Ø£Ø¯Ø®Ù„ Ø¹Ø¯Ø© Ø£Ø¨ÙŠØ§ØªØŒ ÙƒÙ„ Ø¨ÙŠØª ÙÙŠ Ø³Ø·Ø± Ù…Ù†ÙØµÙ„"
        )
    
    col1, col2, col3 = st.columns([1, 1, 1])
    
    with col1:
        analyze_btn = st.button("ğŸ” ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Øµ", use_container_width=True)
    with col2:
        clear_btn = st.button("ğŸ—‘ï¸ Ù…Ø³Ø­", use_container_width=True)
    with col3:
        example_btn = st.button("ğŸ“‹ Ù…Ø«Ø§Ù„", use_container_width=True)
    
    return text, analyze_btn, clear_btn, example_btn

def render_shatr_analysis(analysis: ShatrAnalysis, index: int):
    """Ø¹Ø±Ø¶ Ù†ØªØ§Ø¦Ø¬ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø´Ø·Ø±"""
    with st.container():
        st.markdown(f'<div class="analysis-result">', unsafe_allow_html=True)
        
        # Ø±Ø£Ø³ Ø§Ù„ØªØ­Ù„ÙŠÙ„
        col1, col2, col3 = st.columns([2, 1, 1])
        
        with col1:
            if analysis.is_valid:
                st.success(f"âœ… Ø´Ø·Ø± {index + 1}: {analysis.meter_name} ({analysis.meter_type})")
            else:
                st.error(f"âŒ Ø´Ø·Ø± {index + 1}: Ù„Ù… ÙŠØªÙ… Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„Ø¨Ø­Ø±")
        
        with col2:
            st.metric("Ù†Ø³Ø¨Ø© Ø§Ù„Ø«Ù‚Ø©", f"{analysis.confidence}%")
        
        with col3:
            status = "ØµØ­ÙŠØ­" if analysis.is_valid else "Ø¨Ù‡ Ø£Ø®Ø·Ø§Ø¡"
            st.metric("Ø§Ù„Ø­Ø§Ù„Ø©", status)
        
        # Ø§Ù„Ù†Øµ Ø§Ù„Ø£ØµÙ„ÙŠ
        st.markdown(f'<div class="poem-text">{analysis.original_text}</div>', unsafe_allow_html=True)
        
        # Ø§Ù„ØªÙØ¹ÙŠÙ„Ø§Øª
        if analysis.tafeelat:
            st.markdown("#### ğŸ¯ Ø§Ù„ØªÙØ¹ÙŠÙ„Ø§Øª Ø§Ù„Ù…ÙƒØªØ´ÙØ©:")
            tafeelat_cols = st.columns(len(analysis.tafeelat))
            
            for idx, (col, taf) in enumerate(zip(tafeelat_cols, analysis.tafeelat)):
                with col:
                    completeness = "âœ“" if taf['is_complete'] else "âœ—"
                    st.markdown(f"""
                    <div style="text-align: center; background: rgba(255,255,255,0.1); padding: 10px; border-radius: 10px;">
                        <div style="font-size: 20px; font-weight: bold; color: #f5576c;">{taf['name']}</div>
                        <div style="font-size: 12px; color: rgba(255,255,255,0.8);">{taf['binary']}</div>
                        <div style="font-size: 14px;">{completeness}</div>
                    </div>
                    """, unsafe_allow_html=True)
        
        # Ø§Ù„Ø£ÙƒÙˆØ§Ø¯ Ø§Ù„Ø¹Ø±ÙˆØ¶ÙŠØ©
        with st.expander("ğŸ” Ø§Ù„ØªÙØ§ØµÙŠÙ„ Ø§Ù„ØªÙ‚Ù†ÙŠØ©"):
            col1, col2 = st.columns(2)
            with col1:
                st.code(f"Ø§Ù„Ù†Ù…Ø· Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ: {analysis.binary_code}", language="text")
            with col2:
                st.code(f"Ø§Ù„ÙƒØªØ§Ø¨Ø© Ø§Ù„Ø¹Ø±ÙˆØ¶ÙŠØ©: {analysis.arudi_text}", language="text")
        
        # Ø§Ù„Ø¹Ù„Ù„ ÙˆØ§Ù„Ø£Ø®Ø·Ø§Ø¡
        if analysis.faults:
            st.markdown("#### âš ï¸ Ø§Ù„Ø¹Ù„Ù„ Ø§Ù„Ù…ÙƒØªØ´ÙØ©:")
            for fault in analysis.faults:
                severity_class = "fault-acceptable" if fault.get('severity') == 'acceptable' else "fault-critical"
                st.markdown(f"""
                <span class="fault-badge {severity_class}">
                    {fault.get('type', 'ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ')}: {fault.get('description', '')}
                </span>
                """, unsafe_allow_html=True)
        
        # Ø§Ù„ØªØµØ­ÙŠØ­ Ø§Ù„Ù…Ù‚ØªØ±Ø­
        if analysis.suggested_correction:
            st.info(f"ğŸ’¡ **ØªØµØ­ÙŠØ­ Ù…Ù‚ØªØ±Ø­:** {analysis.suggested_correction}")
        
        st.markdown('</div>', unsafe_allow_html=True)

def render_visualizations(analysis: ShatrAnalysis):
    """Ø¹Ø±Ø¶ Ø§Ù„ØªØµÙˆØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠØ©"""
    if not analysis.tafeelat:
        return
    
    # Ø±Ø³Ù… Ø¨ÙŠØ§Ù†ÙŠ Ù„Ù„ØªÙØ¹ÙŠÙ„Ø§Øª
    tafeela_data = []
    for idx, taf in enumerate(analysis.tafeelat):
        tafeela_data.append({
            'Ø§Ù„ØªÙØ¹ÙŠÙ„Ø©': taf['name'],
            'Ø§Ù„Ù…ÙˆÙ‚Ø¹': idx + 1,
            'Ø§Ù„Ø·ÙˆÙ„': len(taf['binary']),
            'Ù…ÙƒØªÙ…Ù„Ø©': taf['is_complete']
        })
    
    if tafeela_data:
        df = pd.DataFrame(tafeela_data)
        
        fig = px.bar(
            df, 
            x='Ø§Ù„Ù…ÙˆÙ‚Ø¹', 
            y='Ø§Ù„Ø·ÙˆÙ„',
            color='Ù…ÙƒØªÙ…Ù„Ø©',
            text='Ø§Ù„ØªÙØ¹ÙŠÙ„Ø©',
            title='ØªÙˆØ²ÙŠØ¹ Ø§Ù„ØªÙØ¹ÙŠÙ„Ø§Øª ÙÙŠ Ø§Ù„Ø´Ø·Ø±',
            color_discrete_map={True: '#11998e', False: '#f5576c'}
        )
        fig.update_layout(
            plot_bgcolor='rgba(0,0,0,0)',
            paper_bgcolor='rgba(0,0,0,0)',
            font=dict(color='white'),
            title_font_color='white'
        )
        st.plotly_chart(fig, use_container_width=True)
        
        # Ø±Ø³Ù… Ø¨ÙŠØ§Ù†ÙŠ Ø¯Ø§Ø¦Ø±ÙŠ Ù„Ù„Ù†Ù…Ø· Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ
        binary_counts = {'Ù…ØªØ­Ø±Ùƒ (1)': analysis.binary_code.count('1'), 
                      'Ø³Ø§ÙƒÙ† (0)': analysis.binary_code.count('0')}
        
        fig2 = px.pie(
            values=list(binary_counts.values()),
            names=list(binary_counts.keys()),
            title='ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø­Ø±ÙƒØ§Øª ÙˆØ§Ù„Ø³ÙƒÙˆÙ†',
            color_discrete_sequence=['#f5576c', '#11998e']
        )
        fig2.update_layout(
            plot_bgcolor='rgba(0,0,0,0)',
            paper_bgcolor='rgba(0,0,0,0)',
            font=dict(color='white'),
            title_font_color='white'
        )
        st.plotly_chart(fig2, use_container_width=True)

def render_full_poem_analysis(poem_analysis: PoemAnalysis):
    """Ø¹Ø±Ø¶ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù‚ØµÙŠØ¯Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø©"""
    st.markdown("---")
    st.markdown("## ğŸ“Š ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù‚ØµÙŠØ¯Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø©")
    
    # Ù…Ù„Ø®Øµ Ø¹Ø§Ù…
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("Ø§Ù„Ø¨Ø­Ø± Ø§Ù„Ù…ÙˆØ­Ø¯", poem_analysis.unified_meter or "ØºÙŠØ± Ù…Ø­Ø¯Ø¯")
    with col2:
        st.metric("Ø§Ù„Ù†ÙˆØ¹", poem_analysis.meter_type or "ØºÙŠØ± Ù…Ø­Ø¯Ø¯")
    with col3:
        st.metric("Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø´Ø·Ø±", len(poem_analysis.verses))
    with col4:
        st.metric("Ø§Ù„Ø«Ù‚Ø© Ø§Ù„Ø¹Ø§Ù…Ø©", f"{poem_analysis.overall_confidence}%")
    
    # ØªØ­Ù„ÙŠÙ„ ÙƒÙ„ Ø´Ø·Ø±
    for idx, verse in enumerate(poem_analysis.verses):
        render_shatr_analysis(verse, idx)
        if idx < len(poem_analysis.verses) - 1:
            st.markdown("---")

def main():
    """Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
    render_header()
    analysis_mode = render_sidebar()
    
    # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…Ø­Ù„Ù„
    farahidi = FarahidiPro()
    
    # Ø­Ø§Ù„Ø© Ø§Ù„Ø¬Ù„Ø³Ø©
    if 'last_analysis' not in st.session_state:
        st.session_state.last_analysis = None
    
    # Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„
    text, analyze_btn, clear_btn, example_btn = render_analysis_input(analysis_mode)
    
    # Ø£Ø²Ø±Ø§Ø± Ø§Ù„ØªØ­ÙƒÙ…
    if clear_btn:
        st.session_state.last_analysis = None
        st.rerun()
    
    if example_btn:
        if analysis_mode == "Ø´Ø·Ø± ÙˆØ§Ø­Ø¯":
            text = "ÙÙÙ„Ø§ ØªÙØ¸ÙÙ†ÙÙ‘Ù†ÙÙ‘ Ø£ÙÙ†ÙÙ‘ Ø§Ù„Ù„ÙÙ‘ÙŠØ«Ù ÙŠÙØ¨Ù’ØªÙØ³ÙÙ…Ù"
        elif analysis_mode == "Ø¨ÙŠØª ÙƒØ§Ù…Ù„":
            text = "ÙÙÙ„Ø§ ØªÙØ¸ÙÙ†ÙÙ‘Ù†ÙÙ‘ Ø£ÙÙ†ÙÙ‘ Ø§Ù„Ù„ÙÙ‘ÙŠØ«Ù ÙŠÙØ¨Ù’ØªÙØ³ÙÙ…Ù ØŒ ÙˆÙÙ…Ø§ Ø¨ÙØ±ÙØ¯Ù Ø§Ù„ØºÙÙŠØ«Ù Ø¨ÙØ§Ù„Ù…ÙØ±Ù’ØªÙØ¹Ù Ø§Ù„Ù†ÙÙ‘Ù…ÙÙ„Ù"
        else:
            text = """ÙÙÙ„Ø§ ØªÙØ¸ÙÙ†ÙÙ‘Ù†ÙÙ‘ Ø£ÙÙ†ÙÙ‘ Ø§Ù„Ù„ÙÙ‘ÙŠØ«Ù ÙŠÙØ¨Ù’ØªÙØ³ÙÙ…Ù
            ÙˆÙÙ…Ø§ Ø¨ÙØ±ÙØ¯Ù Ø§Ù„ØºÙÙŠØ«Ù Ø¨ÙØ§Ù„Ù…ÙØ±Ù’ØªÙØ¹Ù Ø§Ù„Ù†ÙÙ‘Ù…ÙÙ„Ù
            ÙˆÙÙ„Ø§ Ø§Ù„Ø±ÙÙ‘ÙŠØ­Ù Ù…ÙÙ† Ø¨ÙÙŠÙ†Ù Ø§Ù„Ø£ÙÙŠÙ’Ø¦ÙÙ…Ù ØªÙÙ†ÙØ³ÙÙ‘Ù…Ù"""
        st.rerun()
    
    # Ø§Ù„ØªØ­Ù„ÙŠÙ„
    if analyze_btn and text:
        with st.spinner("Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹Ø±ÙˆØ¶ÙŠ..."):
            if analysis_mode == "Ù‚ØµÙŠØ¯Ø© ÙƒØ§Ù…Ù„Ø©":
                result = farahidi.analyze_full_poem(text)
                st.session_state.last_analysis = result
                render_full_poem_analysis(result)
            elif analysis_mode == "Ø¨ÙŠØª ÙƒØ§Ù…Ù„":
                # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØª ÙˆØªØ­Ù„ÙŠÙ„Ù‡
                parts = text.replace('ØŒ', ' ').split()
                mid = len(parts) // 2
                sadr = ' '.join(parts[:mid])
                ajuz = ' '.join(parts[mid:])
                
                sadr_result = farahidi.analyze_shatr(sadr)
                ajuz_result = farahidi.analyze_shatr(ajuz)
                
                st.markdown("### ğŸ“œ Ø§Ù„ØµØ¯Ø±")
                render_shatr_analysis(sadr_result, 0)
                render_visualizations(sadr_result)
                
                st.markdown("### ğŸ“œ Ø§Ù„Ø¹Ø¬Ø²")
                render_shatr_analysis(ajuz_result, 1)
                render_visualizations(ajuz_result)
            else:
                # Ø´Ø·Ø± ÙˆØ§Ø­Ø¯
                result = farahidi.analyze_shatr(text)
                render_shatr_analysis(result, 0)
                render_visualizations(result)

if __name__ == "__main__":
    main()
